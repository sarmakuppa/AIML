{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f40274",
   "metadata": {},
   "source": [
    "### __Query Decomposition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d02f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import parse\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import torch\n",
    "import tqdm\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['USER_AGENT'] = ''\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\n",
    "                \"post-content\", \"post-title\", \"post-header\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8bf9b",
   "metadata": {},
   "source": [
    "### ___Query LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "Query_LLM = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={ \"temperature\": 0.0,\n",
    "                   \"do_sample\": True },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa571f",
   "metadata": {},
   "source": [
    "### ___Answer LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_LLM = Ollama(\n",
    "    model=\"qwen2.5:7b-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "#print(Answer_LLM(\"Explain transformers in simple terms\"))\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={ \"temperature\": 0.0,\n",
    "#                    \"do_sample\": True },\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a task is to generate multiple search queries related to:\n",
    "{question} \\n\n",
    "Output (3 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | Query_LLM\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "questions = generate_queries_decomposition.invoke( { \"question\": question } )\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e942c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.split()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "answer = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")\n",
    "        }\n",
    "        | decomposition_prompt\n",
    "        | Answer_LLM\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke(\n",
    "        {\n",
    "            \"question\": q, \"q_a_pairs\": q_a_pairs\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "\n",
    "    print(type(q_a_pair))\n",
    "\n",
    "    q_a_pairs = str(q_a_pair) + \"\\n---\\n\" + str(q_a_pair)\n",
    "\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91119f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    sub_questions = sub_question_generator_chain.invoke( { \"question\": question } )\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        answer = (prompt_rag | Answer_LLM | StrOutputParser() ).invoke( { \"context\": retrieved_docs,\n",
    "                                                                         \"question\": sub_question } )\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag( question, prompt_rag, generate_queries_decomposition )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs( questions, answers ):\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start = 1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    \n",
    "    return format_qa_pairs(questions, answers)\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | Answer_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke( { \"context\": context, \"question\": question } )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvForLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
