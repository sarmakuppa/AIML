{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import parse\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import torch\n",
    "import tqdm\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['USER_AGENT'] = ''\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\n",
    "                \"post-content\", \"post-title\", \"post-header\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89d534",
   "metadata": {},
   "source": [
    "### ___Query LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ddb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "Query_LLM = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={ \"temperature\": 0.0,\n",
    "                   \"do_sample\": True },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92d08e",
   "metadata": {},
   "source": [
    "### ___Answer LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ec670",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_LLM = Ollama(\n",
    "    model=\"qwen2.5:7b-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "#print(Answer_LLM(\"Explain transformers in simple terms\"))\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={ \"temperature\": 0.0,\n",
    "#                    \"do_sample\": True },\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc1037",
   "metadata": {},
   "source": [
    "### __Multi Query__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\n",
    "                \"post-content\", \"post-title\", \"post-header\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54175841",
   "metadata": {},
   "source": [
    "### ___Prompt___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Multi Query\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions of the given\n",
    "user question to retrieve relevant documents from a vector database. By generating multiple perspectives\n",
    "on the user question, your goal is to help the user overcome some of the limitations of the distance\n",
    "based similarity search. Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def parse_queries(text: str) -> list[str]:\n",
    "    return [\n",
    "        line.strip()\n",
    "        for line in text.split(\"\\n\")\n",
    "        if line.strip()\n",
    "    ]\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "query_chain = (\n",
    "    prompt_perspectives\n",
    "    | Query_LLM\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = query_chain | retriever.map() | get_unique_union\n",
    "\n",
    "docs = retrieval_chain.invoke( { \"question\" : question } )\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "final_rag_chain = (\n",
    "    { \"context\": retrieval_chain,\n",
    "      \"question\": itemgetter(\"question\") }\n",
    "    | prompt\n",
    "    | Answer_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke( {\"question\": question} )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
