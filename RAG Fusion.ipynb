{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbeb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import parse\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import torch\n",
    "import tqdm\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['USER_AGENT'] = ''\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\n",
    "                \"post-content\", \"post-title\", \"post-header\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779fc6e",
   "metadata": {},
   "source": [
    "### ___Query LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc73cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e5a3a1219e4d1eaf9d952c425e087f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "Query_LLM = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={ \"temperature\": 0.0,\n",
    "                   \"do_sample\": True },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ff1cf",
   "metadata": {},
   "source": [
    "### ___Answer LLM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe803204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarma\\AppData\\Local\\Temp\\ipykernel_14912\\3409403279.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  Answer_LLM = Ollama(\n",
      "C:\\Users\\sarma\\AppData\\Local\\Temp\\ipykernel_14912\\3409403279.py:6: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(Answer_LLM(\"Explain transformers in simple terms\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Transformers are a type of neural network architecture used primarily for processing sequences of data, like text or speech. They were introduced in 2017 and have since become very popular because they can handle tasks more efficiently than previous models.\n",
      "\n",
      "Here’s a simple explanation:\n",
      "\n",
      "Imagine you're reading a book. To understand the story, you need to remember what happened earlier while focusing on the current page. Traditional methods would require you to reread parts of the book multiple times or keep notes, which is time-consuming.\n",
      "\n",
      "Transformers work differently. They use something called \"self-attention,\" which allows them to focus on different words in a sentence based on their importance and context. It's like being able to instantly recall any part of the story without needing to reread it.\n",
      "\n",
      "Here’s how it works:\n",
      "1. **Input**: You give the transformer a sequence, like a sentence.\n",
      "2. **Self-Attention**: The model looks at each word and decides which other words in the sentence are most relevant to understand that particular word's meaning.\n",
      "3. **Processing**: It then combines this information to create a new representation of each word.\n",
      "4. **Output**: Finally, it produces an output based on these processed representations.\n",
      "\n",
      "This approach makes transformers very efficient because they can process all parts of the input at once without needing to reprocess earlier data. They’re used in many applications like translating languages, understanding text for chatbots, and even generating new text.\n",
      "\n",
      "In summary, transformers are powerful tools that help machines understand and generate human-like language by focusing on relevant information within a sequence.\n"
     ]
    }
   ],
   "source": [
    "Answer_LLM = Ollama(\n",
    "    model=\"qwen2.5:7b-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "#print(Answer_LLM(\"Explain transformers in simple terms\"))\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={ \"temperature\": 0.0,\n",
    "#                    \"do_sample\": True },\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# Answer_LLM = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84af42",
   "metadata": {},
   "source": [
    "### __RAG-Fusion__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599734d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an AI language model assistant. Generate multiple search queries related \n",
    "Original question: {question}\n",
    "output (4 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | Query_LLM\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k = 60):\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "        \n",
    "        reranked_results = [\n",
    "            (load(doc), score)\n",
    "            for doc, score in sorted(fused_scores.items(), key = lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "\n",
    "        return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke( { \"question\": question } )\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3741b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    { \"context\" :  retrieval_chain_rag_fusion,\n",
    "      \"question\": itemgetter(\"question\")\n",
    "     }\n",
    "    | prompt\n",
    "    | Answer_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke( { \"question\": question } )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvForLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
